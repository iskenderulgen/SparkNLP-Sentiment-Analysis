{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "from pyspark.sql import SparkSession\n",
    "from sparknlp.base import DocumentAssembler, Pipeline, EmbeddingsFinisher\n",
    "from sparknlp.annotator import Tokenizer, WordEmbeddingsModel, SentenceEmbeddings\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Spark-nlp downloads huge amount of jar files, this might take a while\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Spark-Text-Classification\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"8G\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.5.3\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/stanfordnlp/imdb\n",
    "\n",
    "imdb_dataset = spark.read.parquet(\"data/imdb-train-00000-of-00001.parquet\").withColumn(\n",
    "    \"text\", regexp_replace(col(\"text\"), \"[^a-zA-Z0-9\\\\s]\", \"\")\n",
    ")\n",
    "\n",
    "# Create a window partitioned by label and ordered randomly\n",
    "windowSpec = Window.partitionBy(\"label\").orderBy(F.rand())\n",
    "\n",
    "\"\"\"\n",
    "# Add a row number per label and filter to keep only the first 1000 rows per class\n",
    "# This ensures for efficiency and balance in the dataset for testing. \n",
    "# For production, you might want to use full dataset\n",
    "\"\"\"\n",
    "imdb_dataset = (\n",
    "    imdb_dataset.withColumn(\"row_num\", F.row_number().over(windowSpec))\n",
    "    .filter(F.col(\"row_num\") <= 1000)\n",
    "    .drop(\"row_num\")\n",
    ")\n",
    "\n",
    "# Show the sampled dataset\n",
    "imdb_dataset.show(5, truncate=50)\n",
    "\n",
    "print(\n",
    "    \"Number of classes in the sampled dataset: \",\n",
    "    imdb_dataset.select(\"label\").distinct().count(),\n",
    "    \"total number of rows: \",\n",
    "    imdb_dataset.count(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a pipeline that cleans and tokenizes the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Spark NLP pipeline stages\n",
    "\n",
    "# 1. DocumentAssembler converts raw text into a document annotation.\n",
    "document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "\n",
    "# 2. Tokenizer splits the document into tokens.\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
    "\n",
    "# 3. Load pre-trained GloVe embeddings\n",
    "# 3. Load GloVe embeddings from local file\n",
    "word_embeddings = (\n",
    "    WordEmbeddingsModel.load(\"data/glove_100d\")\n",
    "    .setInputCols([\"document\", \"token\"])\n",
    "    .setOutputCol(\"embeddings\")\n",
    ")\n",
    "\n",
    "# 4. Create sentence-level embeddings by averaging the word embeddings.\n",
    "sentence_embeddings = (\n",
    "    SentenceEmbeddings()\n",
    "    .setInputCols([\"document\", \"embeddings\"])\n",
    "    .setOutputCol(\"sentence_embeddings\")\n",
    "    .setPoolingStrategy(\"AVERAGE\")\n",
    ")\n",
    "\n",
    "# 5. Finisher converts NLP annotations into plain array column\n",
    "finisher = (\n",
    "    EmbeddingsFinisher()\n",
    "    .setInputCols(\"sentence_embeddings\")\n",
    "    .setOutputCols(\"features\")\n",
    "    .setOutputAsVector(True)\n",
    "    .setCleanAnnotations(True)\n",
    ")\n",
    "\n",
    "# Build the pipeline\n",
    "nlp_pipeline = Pipeline(\n",
    "    stages=[\n",
    "        document_assembler,\n",
    "        tokenizer,\n",
    "        word_embeddings,\n",
    "        sentence_embeddings,\n",
    "        finisher,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the data\n",
    "final_data = (\n",
    "    nlp_pipeline.fit(imdb_dataset)\n",
    "    .transform(imdb_dataset)\n",
    "    .selectExpr(\"label\", \"explode(features) as features\")\n",
    ")\n",
    "\n",
    "final_data.show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-Train a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training (80%) and testing (20%) sets\n",
    "train_data, test_data = final_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Configure the LogisticRegression model from spark-rapids-ml\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=50)\n",
    "\n",
    "# Fit the model using the training data\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate test accuracy; compare the predicted labels with numeric labels (\"label_index\")\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-Use Trained model on Unseen Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "we have used Yelp reviews for outbound sentiment analysis https://huggingface.co/datasets/Yelp/yelp_review_full\n",
    "Yelp dataset are presentedin train-test split which can also be used for \n",
    "trainin multiclass classification model sinde it has 5 classes for each review\n",
    "for simplicity we have used only 2 classes for binary classification using IMDB dataset\n",
    "\"\"\"\n",
    "\n",
    "# Read the Yelp dataset from parquet file\n",
    "yelp_dataset = (\n",
    "    spark.read.parquet(\"data/yelp-test-00000-of-00001.parquet\")\n",
    "    .select(\"text\")\n",
    "    .withColumn(\"text\", regexp_replace(col(\"text\"), \"[^a-zA-Z0-9\\\\s]\", \"\"))\n",
    ")\n",
    "\n",
    "# Show a sample of the data\n",
    "yelp_dataset.show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1-Use NLP Pipeline to convert sentences into 100dim embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_dataset = (\n",
    "    nlp_pipeline.fit(yelp_dataset)\n",
    "    .transform(yelp_dataset)\n",
    "    .selectExpr(\"explode(features) as features\")\n",
    ")\n",
    "\n",
    "yelp_dataset.show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2-Use trained regression model to classify unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_predictions = model.transform(yelp_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_predictions.show(5, truncate=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
